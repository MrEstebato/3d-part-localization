{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983687be",
   "metadata": {},
   "source": [
    "# Train GCN for Heat Stake Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5421503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8c6c074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sm_50', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'sm_90']\n",
      "True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 60\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.001\n",
    "DROPOUT = 0.3\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(torch.cuda.get_arch_list())\n",
    "print(torch.cuda.is_available())\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce416e45",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24ca4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150 heatstake STEP files and 158 other STEP files.\n",
      "Saved dataset with 308 graphs to c:\\Users\\A01369877\\Documents\\GM\\3d-part-localization\\GCN\\training_ready_dataset.pt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cadquery as cq\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "\n",
    "# Resolve paths relative to this notebook's folder when possible\n",
    "BASE_DIR = Path.cwd().parent\n",
    "if str(BASE_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(BASE_DIR))\n",
    "    \n",
    "from preprocessing.graphs import build_brep_graph, nx_to_PyG\n",
    "\n",
    "DATA_DIR = BASE_DIR / \"GCN\" / \"training_data\"\n",
    "HEATSTAKE_DIR = DATA_DIR / \"allheatstakes\"\n",
    "OTHER_DIR = DATA_DIR / \"allother\"\n",
    "DATASET_FILE = BASE_DIR / \"GCN\" / \"training_ready_dataset.pt\"\n",
    "\n",
    "if True:\n",
    "    dataset = []\n",
    "\n",
    "    def iter_step_files(folder: Path):\n",
    "        return [p for p in folder.rglob('*') if p.suffix.lower() in {'.stp', '.step'}]\n",
    "\n",
    "    possible_heatstakes = iter_step_files(HEATSTAKE_DIR)\n",
    "    possible_others = iter_step_files(OTHER_DIR)\n",
    "\n",
    "    print(f\"Found {len(possible_heatstakes)} heatstake STEP files and {len(possible_others)} other STEP files.\")\n",
    "    for heatstake_path in possible_heatstakes:\n",
    "        solids = cq.importers.importStep(str(heatstake_path)).faces()\n",
    "        G = build_brep_graph(solids)\n",
    "        data = nx_to_PyG([G])\n",
    "        data[0].y = torch.tensor([1], dtype=torch.long)  # class 1 = heatstake\n",
    "        dataset.append(data[0])\n",
    "    for other_path in possible_others:\n",
    "        solids = cq.importers.importStep(str(other_path)).faces()    \n",
    "        G = build_brep_graph(solids)\n",
    "        data = nx_to_PyG([G])\n",
    "        data[0].y = torch.tensor([0], dtype=torch.long)  # class 0 = other\n",
    "        dataset.append(data[0])\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        print(\"No graphs were created. Ensure your folders contain .stp/.step files and your preprocessing functions are available.\")\n",
    "    else:\n",
    "        torch.save(dataset, DATASET_FILE)\n",
    "        print(f\"Saved dataset with {len(dataset)} graphs to {DATASET_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10664931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 308 graphs from c:\\Users\\A01369877\\Documents\\GM\\3d-part-localization\\GCN\\training_ready_dataset.pt\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (expects a single .pt file saved as a list of PyG Data objects)\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATASET_FILE = BASE_DIR / \"GCN\" / \"training_ready_dataset.pt\"\n",
    "\n",
    "if DATASET_FILE.exists():\n",
    "    dataset = torch.load(DATASET_FILE, weights_only=False)\n",
    "    print(f\"Loaded dataset with {len(dataset)} graphs from {DATASET_FILE}\")\n",
    "else:\n",
    "    dataset = []\n",
    "    print(f\"Dataset file not found at {DATASET_FILE}. Add data or build dataset first.\")\n",
    "\n",
    "# Basic sanity check\n",
    "if len(dataset) > 0:\n",
    "    assert hasattr(dataset[0], 'x') and hasattr(dataset[0], 'edge_index') and hasattr(dataset[0], 'y'), \\\n",
    "        \"Each Data must have x, edge_index, and y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db54de4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train graphs: 246 | Val graphs: 62\n"
     ]
    }
   ],
   "source": [
    "# Split into train/val and create loaders\n",
    "if len(dataset) > 0:\n",
    "    labels = [int(d.y.item()) for d in dataset]\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        list(range(len(dataset))),\n",
    "        test_size=VAL_SPLIT,\n",
    "        random_state=SEED,\n",
    "        stratify=labels if len(set(labels)) > 1 else None,\n",
    "    )\n",
    "    train_dataset = [dataset[i] for i in train_idx]\n",
    "    val_dataset = [dataset[i] for i in val_idx]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    print(f\"Train graphs: {len(train_dataset)} | Val graphs: {len(val_dataset)}\")\n",
    "else:\n",
    "    train_loader = None\n",
    "    val_loader = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cb3f935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN2(\n",
      "  (convs): ModuleList(\n",
      "    (0): GCNConv(3, 64)\n",
      "    (1-2): 2 x GCNConv(64, 64)\n",
      "  )\n",
      "  (norms): ModuleList(\n",
      "    (0-2): 3 x LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from GCN import GCN2\n",
    "\n",
    "# Create model, criterion, optimizer\n",
    "if train_loader is not None:\n",
    "    in_channels = train_dataset[0].x.size(-1)\n",
    "    #model = GCN(feature_dim_size=in_channels, num_classes=2, dropout=DROPOUT).to(DEVICE)\n",
    "    model = GCN2(feature_dim_size=in_channels, num_classes=2, dropout=DROPOUT).to(DEVICE)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    print(model)\n",
    "else:\n",
    "    model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac6309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/eval helpers\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        # The provided GCN does not aggregate per-graph using the batch vector, so\n",
    "        # we process each graph in the batch individually.\n",
    "        data_list = batch.to_data_list()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = 0.0\n",
    "        batch_correct = 0\n",
    "        batch_total = 0\n",
    "\n",
    "        for data in data_list:\n",
    "            data = data.to(DEVICE)\n",
    "            out = model(adj=data.edge_index, features=data.x)  # shape [1, 2]\n",
    "            loss = criterion(out, data.y.long())\n",
    "            loss.backward()\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "            preds = out.argmax(dim=1)\n",
    "            batch_correct += int((preds == data.y).sum().item())\n",
    "            batch_total += data.y.size(0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        correct += batch_correct\n",
    "        total += batch_total\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(loader))\n",
    "    acc = correct / max(1, total)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            data_list = batch.to_data_list()\n",
    "            batch_loss = 0.0\n",
    "            batch_correct = 0\n",
    "            batch_total = 0\n",
    "            for data in data_list:\n",
    "                data = data.to(DEVICE)\n",
    "                out = model(adj=data.edge_index, features=data.x)\n",
    "                loss = criterion(out, data.y.long())\n",
    "                batch_loss += loss.item()\n",
    "                preds = out.argmax(dim=1)\n",
    "                batch_correct += int((preds == data.y).sum().item())\n",
    "                batch_total += data.y.size(0)\n",
    "            total_loss += batch_loss\n",
    "            correct += batch_correct\n",
    "            total += batch_total\n",
    "    avg_loss = total_loss / max(1, len(loader))\n",
    "    acc = correct / max(1, total)\n",
    "    return avg_loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fce4c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Loss: 11.9014 Acc: 0.500 | Val Loss: 10.6723 Acc: 0.516\n",
      "Epoch 002 | Train Loss: 10.9981 Acc: 0.520 | Val Loss: 10.6824 Acc: 0.516\n",
      "Epoch 003 | Train Loss: 10.5924 Acc: 0.541 | Val Loss: 10.7756 Acc: 0.516\n",
      "Epoch 004 | Train Loss: 10.7622 Acc: 0.520 | Val Loss: 10.6867 Acc: 0.516\n",
      "Epoch 005 | Train Loss: 10.5378 Acc: 0.581 | Val Loss: 10.6342 Acc: 0.516\n",
      "Epoch 006 | Train Loss: 10.5434 Acc: 0.553 | Val Loss: 10.6443 Acc: 0.516\n",
      "Epoch 007 | Train Loss: 10.6237 Acc: 0.549 | Val Loss: 10.6476 Acc: 0.516\n",
      "Epoch 008 | Train Loss: 10.6386 Acc: 0.516 | Val Loss: 10.6375 Acc: 0.516\n",
      "Epoch 009 | Train Loss: 10.6017 Acc: 0.500 | Val Loss: 10.6684 Acc: 0.516\n",
      "Epoch 010 | Train Loss: 10.4195 Acc: 0.577 | Val Loss: 10.6237 Acc: 0.516\n",
      "Epoch 011 | Train Loss: 10.3560 Acc: 0.598 | Val Loss: 10.6285 Acc: 0.516\n",
      "Epoch 012 | Train Loss: 10.3250 Acc: 0.602 | Val Loss: 10.4921 Acc: 0.516\n",
      "Epoch 013 | Train Loss: 10.3109 Acc: 0.618 | Val Loss: 10.3939 Acc: 0.516\n",
      "Epoch 014 | Train Loss: 10.2364 Acc: 0.606 | Val Loss: 10.2475 Acc: 0.516\n",
      "Epoch 015 | Train Loss: 9.9676 Acc: 0.667 | Val Loss: 10.4473 Acc: 0.516\n",
      "Epoch 016 | Train Loss: 9.6037 Acc: 0.687 | Val Loss: 9.6880 Acc: 0.629\n",
      "Epoch 017 | Train Loss: 9.3194 Acc: 0.691 | Val Loss: 9.2791 Acc: 0.871\n",
      "Epoch 018 | Train Loss: 8.9009 Acc: 0.703 | Val Loss: 8.3654 Acc: 0.871\n",
      "Epoch 019 | Train Loss: 7.9674 Acc: 0.809 | Val Loss: 7.5424 Acc: 0.871\n",
      "Epoch 020 | Train Loss: 7.1261 Acc: 0.805 | Val Loss: 6.7501 Acc: 0.855\n",
      "Epoch 021 | Train Loss: 7.2751 Acc: 0.809 | Val Loss: 6.0898 Acc: 0.871\n",
      "Epoch 022 | Train Loss: 6.9443 Acc: 0.813 | Val Loss: 6.1483 Acc: 0.871\n",
      "Epoch 023 | Train Loss: 7.1046 Acc: 0.813 | Val Loss: 5.9040 Acc: 0.871\n",
      "Epoch 024 | Train Loss: 6.4600 Acc: 0.825 | Val Loss: 6.2522 Acc: 0.871\n",
      "Epoch 025 | Train Loss: 6.9437 Acc: 0.817 | Val Loss: 5.8826 Acc: 0.871\n",
      "Epoch 026 | Train Loss: 7.2168 Acc: 0.793 | Val Loss: 6.4797 Acc: 0.871\n",
      "Epoch 027 | Train Loss: 6.5842 Acc: 0.841 | Val Loss: 6.1353 Acc: 0.855\n",
      "Epoch 028 | Train Loss: 6.4896 Acc: 0.837 | Val Loss: 5.9462 Acc: 0.871\n",
      "Epoch 029 | Train Loss: 5.6712 Acc: 0.854 | Val Loss: 5.7443 Acc: 0.855\n",
      "Epoch 030 | Train Loss: 6.5933 Acc: 0.817 | Val Loss: 6.7501 Acc: 0.823\n",
      "Epoch 031 | Train Loss: 5.6360 Acc: 0.866 | Val Loss: 6.5135 Acc: 0.823\n",
      "Epoch 032 | Train Loss: 6.5644 Acc: 0.805 | Val Loss: 5.8921 Acc: 0.871\n",
      "Epoch 033 | Train Loss: 5.9809 Acc: 0.858 | Val Loss: 5.7937 Acc: 0.871\n",
      "Epoch 034 | Train Loss: 6.2386 Acc: 0.846 | Val Loss: 5.7518 Acc: 0.871\n",
      "Epoch 035 | Train Loss: 6.1233 Acc: 0.841 | Val Loss: 5.7425 Acc: 0.871\n",
      "Epoch 036 | Train Loss: 6.0992 Acc: 0.841 | Val Loss: 5.7255 Acc: 0.871\n",
      "Epoch 037 | Train Loss: 6.0711 Acc: 0.841 | Val Loss: 5.4410 Acc: 0.871\n",
      "Epoch 038 | Train Loss: 5.9732 Acc: 0.841 | Val Loss: 5.4638 Acc: 0.871\n",
      "Epoch 039 | Train Loss: 6.1459 Acc: 0.850 | Val Loss: 5.7367 Acc: 0.871\n",
      "Epoch 040 | Train Loss: 5.5605 Acc: 0.841 | Val Loss: 5.3595 Acc: 0.871\n",
      "Epoch 041 | Train Loss: 5.8958 Acc: 0.850 | Val Loss: 5.4415 Acc: 0.871\n",
      "Epoch 042 | Train Loss: 5.6395 Acc: 0.862 | Val Loss: 5.4961 Acc: 0.871\n",
      "Epoch 043 | Train Loss: 6.0856 Acc: 0.841 | Val Loss: 5.4310 Acc: 0.871\n",
      "Epoch 044 | Train Loss: 5.5134 Acc: 0.846 | Val Loss: 5.4492 Acc: 0.871\n",
      "Epoch 045 | Train Loss: 5.8287 Acc: 0.837 | Val Loss: 5.3222 Acc: 0.871\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m history \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: []}\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     tr_loss, tr_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     vl_loss, vl_acc \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion)\n\u001b[0;32m      8\u001b[0m     history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(tr_loss)\n",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_list:\n\u001b[0;32m     20\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 21\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape [1, 2]\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m     23\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\Documents\\GM\\3d-part-localization\\GCN\\GCN.py:133\u001b[0m, in \u001b[0;36mGCN2.forward\u001b[1;34m(self, adj, features)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv, norm \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorms):\n\u001b[0;32m    132\u001b[0m     h_in \u001b[38;5;241m=\u001b[39m h\n\u001b[1;32m--> 133\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# GCN layer\u001b[39;00m\n\u001b[0;32m    134\u001b[0m     h \u001b[38;5;241m=\u001b[39m norm(h)  \u001b[38;5;66;03m# normalization\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(h)\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    239\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 241\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:99\u001b[0m, in \u001b[0;36mgcn_norm\u001b[1;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_self_loops:\n\u001b[1;32m---> 99\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43madd_remaining_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), ), dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    104\u001b[0m                              device\u001b[38;5;241m=\u001b[39medge_index\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch_geometric\\utils\\loop.py:652\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[1;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[0;32m    648\u001b[0m     is_undirected \u001b[38;5;241m=\u001b[39m edge_index\u001b[38;5;241m.\u001b[39mis_undirected\n\u001b[0;32m    650\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m edge_index[:, mask]\n\u001b[1;32m--> 652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_scripting\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, EdgeIndex):\n\u001b[0;32m    653\u001b[0m     edge_index\u001b[38;5;241m.\u001b[39m_is_undirected \u001b[38;5;241m=\u001b[39m is_undirected\n\u001b[0;32m    655\u001b[0m edge_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([edge_index, loop_index], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch\\_jit_internal.py:103\u001b[0m, in \u001b[0;36mis_scripting\u001b[1;34m()\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m7\u001b[39m):\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcastingList\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m BroadcastingList1\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_scripting\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    Function that returns True when in compilation and False otherwise. This\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    is useful especially with the @unused decorator to leave code in your\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m                return unsupported_linear_op(x)\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop with plots\n",
    "if train_loader is not None and model is not None:\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        vl_loss, vl_acc = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_loss\"].append(vl_loss)\n",
    "        history[\"val_acc\"].append(vl_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | Train Loss: {tr_loss:.4f} Acc: {tr_acc:.3f} | Val Loss: {vl_loss:.4f} Acc: {vl_acc:.3f}\")\n",
    "\n",
    "    # Plot loss and accuracy\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    axs[0].plot(history[\"train_loss\"], label=\"train\")\n",
    "    axs[0].plot(history[\"val_loss\"], label=\"val\")\n",
    "    axs[0].set_title(\"Loss\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"NLLLoss\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(history[\"train_acc\"], label=\"train\")\n",
    "    axs[1].plot(history[\"val_acc\"], label=\"val\")\n",
    "    axs[1].set_title(\"Accuracy\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No dataset loaded. Build or load heatstake_dataset.pt first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03d80868",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), BASE_DIR / \"GCN\" / \"heatstake_classifier.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heatstakes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
