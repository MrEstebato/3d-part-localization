{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983687be",
   "metadata": {},
   "source": [
    "# Train GCN for Heat Stake Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5421503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8c6c074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sm_50', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'sm_90']\n",
      "True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 60\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.001\n",
    "DROPOUT = 0.3\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(torch.cuda.get_arch_list())\n",
    "print(torch.cuda.is_available())\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce416e45",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b24ca4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150 heatstake STEP files and 158 other STEP files.\n",
      "Saved dataset with 308 graphs to c:\\Users\\A01369877\\Documents\\GM\\3d-part-localization\\GCN\\training_ready_dataset.pt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cadquery as cq\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "\n",
    "# Resolve paths relative to this notebook's folder when possible\n",
    "BASE_DIR = Path.cwd().parent\n",
    "if str(BASE_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(BASE_DIR))\n",
    "    \n",
    "from preprocessing.graphs import build_brep_graph, nx_to_PyG\n",
    "\n",
    "DATA_DIR = BASE_DIR / \"GCN\" / \"training_data\"\n",
    "HEATSTAKE_DIR = DATA_DIR / \"allheatstakes\"\n",
    "OTHER_DIR = DATA_DIR / \"allother\"\n",
    "DATASET_FILE = BASE_DIR / \"GCN\" / \"training_ready_dataset.pt\"\n",
    "\n",
    "if True:\n",
    "    dataset = []\n",
    "\n",
    "    def iter_step_files(folder: Path):\n",
    "        return [p for p in folder.rglob('*') if p.suffix.lower() in {'.stp', '.step'}]\n",
    "\n",
    "    possible_heatstakes = iter_step_files(HEATSTAKE_DIR)\n",
    "    possible_others = iter_step_files(OTHER_DIR)\n",
    "\n",
    "    print(f\"Found {len(possible_heatstakes)} heatstake STEP files and {len(possible_others)} other STEP files.\")\n",
    "    for heatstake_path in possible_heatstakes:\n",
    "        solids = cq.importers.importStep(str(heatstake_path)).faces()\n",
    "        G = build_brep_graph(solids)\n",
    "        data = nx_to_PyG([G])\n",
    "        data[0].y = torch.tensor([1], dtype=torch.long)  # class 1 = heatstake\n",
    "        dataset.append(data[0])\n",
    "    for other_path in possible_others:\n",
    "        solids = cq.importers.importStep(str(other_path)).faces()    \n",
    "        G = build_brep_graph(solids)\n",
    "        data = nx_to_PyG([G])\n",
    "        data[0].y = torch.tensor([0], dtype=torch.long)  # class 0 = other\n",
    "        dataset.append(data[0])\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        print(\"No graphs were created. Ensure your folders contain .stp/.step files and your preprocessing functions are available.\")\n",
    "    else:\n",
    "        torch.save(dataset, DATASET_FILE)\n",
    "        print(f\"Saved dataset with {len(dataset)} graphs to {DATASET_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10664931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 308 graphs from c:\\Users\\A01369877\\Documents\\GM\\3d-part-localization\\GCN\\training_ready_dataset.pt\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (expects a single .pt file saved as a list of PyG Data objects)\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATASET_FILE = BASE_DIR / \"GCN\" / \"training_ready_dataset.pt\"\n",
    "\n",
    "if DATASET_FILE.exists():\n",
    "    dataset = torch.load(DATASET_FILE, weights_only=False)\n",
    "    print(f\"Loaded dataset with {len(dataset)} graphs from {DATASET_FILE}\")\n",
    "else:\n",
    "    dataset = []\n",
    "    print(f\"Dataset file not found at {DATASET_FILE}. Add data or build dataset first.\")\n",
    "\n",
    "# Basic sanity check\n",
    "if len(dataset) > 0:\n",
    "    assert hasattr(dataset[0], 'x') and hasattr(dataset[0], 'edge_index') and hasattr(dataset[0], 'y'), \\\n",
    "        \"Each Data must have x, edge_index, and y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db54de4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train graphs: 246 | Val graphs: 62\n"
     ]
    }
   ],
   "source": [
    "# Split into train/val and create loaders\n",
    "if len(dataset) > 0:\n",
    "    labels = [int(d.y.item()) for d in dataset]\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        list(range(len(dataset))),\n",
    "        test_size=VAL_SPLIT,\n",
    "        random_state=SEED,\n",
    "        stratify=labels if len(set(labels)) > 1 else None,\n",
    "    )\n",
    "    train_dataset = [dataset[i] for i in train_idx]\n",
    "    val_dataset = [dataset[i] for i in val_idx]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    print(f\"Train graphs: {len(train_dataset)} | Val graphs: {len(val_dataset)}\")\n",
    "else:\n",
    "    train_loader = None\n",
    "    val_loader = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cb3f935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (convolution_1): GCNConv(3, 64)\n",
      "  (convolution_2): GCNConv(64, 32)\n",
      "  (convolution_3): GCNConv(32, 32)\n",
      "  (attention): AttentionModule()\n",
      "  (fully_connected_first): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fully_connected_second): Linear(in_features=32, out_features=8, bias=True)\n",
      "  (scoring_layer): Linear(in_features=8, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from GCN import GCN\n",
    "\n",
    "# Create model, criterion, optimizer\n",
    "if train_loader is not None:\n",
    "    in_channels = train_dataset[0].x.size(-1)\n",
    "    model = GCN(feature_dim_size=in_channels, num_classes=2, dropout=DROPOUT).to(DEVICE)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    print(model)\n",
    "else:\n",
    "    model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ac6309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/eval helpers\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        # The provided GCN does not aggregate per-graph using the batch vector, so\n",
    "        # we process each graph in the batch individually.\n",
    "        data_list = batch.to_data_list()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = 0.0\n",
    "        batch_correct = 0\n",
    "        batch_total = 0\n",
    "\n",
    "        for data in data_list:\n",
    "            data = data.to(DEVICE)\n",
    "            out = model(adj=data.edge_index, features=data.x)  # shape [1, 2]\n",
    "            loss = criterion(out, data.y.long())\n",
    "            loss.backward()\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "            preds = out.argmax(dim=1)\n",
    "            batch_correct += int((preds == data.y).sum().item())\n",
    "            batch_total += data.y.size(0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        correct += batch_correct\n",
    "        total += batch_total\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(loader))\n",
    "    acc = correct / max(1, total)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            data_list = batch.to_data_list()\n",
    "            batch_loss = 0.0\n",
    "            batch_correct = 0\n",
    "            batch_total = 0\n",
    "            for data in data_list:\n",
    "                data = data.to(DEVICE)\n",
    "                out = model(adj=data.edge_index, features=data.x)\n",
    "                loss = criterion(out, data.y.long())\n",
    "                batch_loss += loss.item()\n",
    "                preds = out.argmax(dim=1)\n",
    "                batch_correct += int((preds == data.y).sum().item())\n",
    "                batch_total += data.y.size(0)\n",
    "            total_loss += batch_loss\n",
    "            correct += batch_correct\n",
    "            total += batch_total\n",
    "    avg_loss = total_loss / max(1, len(loader))\n",
    "    acc = correct / max(1, total)\n",
    "    return avg_loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fce4c73",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m history \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: []}\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     tr_loss, tr_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     vl_loss, vl_acc \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion)\n\u001b[0;32m      8\u001b[0m     history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(tr_loss)\n",
      "Cell \u001b[1;32mIn[24], line 21\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_list:\n\u001b[0;32m     20\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 21\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape [1, 2]\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m     23\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\Documents\\GM\\3d-part-localization\\GCN\\GCN.py:68\u001b[0m, in \u001b[0;36mGCN.forward\u001b[1;34m(self, adj, features)\u001b[0m\n\u001b[0;32m     64\u001b[0m features \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(features)\n\u001b[0;32m     65\u001b[0m features \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(\n\u001b[0;32m     66\u001b[0m     features, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[0;32m     67\u001b[0m )\n\u001b[1;32m---> 68\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolution_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m features \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(features)\n\u001b[0;32m     70\u001b[0m features \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(\n\u001b[0;32m     71\u001b[0m     features, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[0;32m     72\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    239\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 241\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:99\u001b[0m, in \u001b[0;36mgcn_norm\u001b[1;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_self_loops:\n\u001b[1;32m---> 99\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43madd_remaining_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), ), dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    104\u001b[0m                              device\u001b[38;5;241m=\u001b[39medge_index\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch_geometric\\utils\\loop.py:634\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[1;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[0;32m    628\u001b[0m     loop_index: Tensor \u001b[38;5;241m=\u001b[39m EdgeIndex(\n\u001b[0;32m    629\u001b[0m         torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, N, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    630\u001b[0m         sparse_size\u001b[38;5;241m=\u001b[39m(N, N),\n\u001b[0;32m    631\u001b[0m         is_undirected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    632\u001b[0m     )\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 634\u001b[0m     loop_index \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    638\u001b[0m     loop_attr \u001b[38;5;241m=\u001b[39m compute_loop_attr(  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    639\u001b[0m         edge_index, edge_attr, N, \u001b[38;5;28;01mFalse\u001b[39;00m, fill_value)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop with plots\n",
    "if train_loader is not None and model is not None:\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        vl_loss, vl_acc = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_loss\"].append(vl_loss)\n",
    "        history[\"val_acc\"].append(vl_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | Train Loss: {tr_loss:.4f} Acc: {tr_acc:.3f} | Val Loss: {vl_loss:.4f} Acc: {vl_acc:.3f}\")\n",
    "\n",
    "    # Plot loss and accuracy\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    axs[0].plot(history[\"train_loss\"], label=\"train\")\n",
    "    axs[0].plot(history[\"val_loss\"], label=\"val\")\n",
    "    axs[0].set_title(\"Loss\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"NLLLoss\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(history[\"train_acc\"], label=\"train\")\n",
    "    axs[1].plot(history[\"val_acc\"], label=\"val\")\n",
    "    axs[1].set_title(\"Accuracy\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No dataset loaded. Build or load heatstake_dataset.pt first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03d80868",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), BASE_DIR / \"GCN\" / \"heatstake_classifier.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heatstakes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
