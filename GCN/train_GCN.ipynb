{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983687be",
   "metadata": {},
   "source": [
    "# Train GCN for Heat Stake Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5421503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c6c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.001\n",
    "DROPOUT = 0.3\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce416e45",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b24ca4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 heatstake STEP files and 0 other STEP files.\n",
      "No graphs were created. Ensure your folders contain .stp/.step files and your preprocessing functions are available.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import cadquery as cq\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "\n",
    "# Resolve paths relative to this notebook's folder when possible\n",
    "BASE_DIR = Path.cwd().parent\n",
    "if str(BASE_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(BASE_DIR))\n",
    "    \n",
    "from preprocessing.graphs import build_brep_graph, nx_to_PyG\n",
    "\n",
    "DATA_DIR = BASE_DIR / \"training_data\"\n",
    "HEATSTAKE_DIR = DATA_DIR / \"heatstakes\"\n",
    "OTHER_DIR = DATA_DIR / \"other\"\n",
    "DATASET_FILE = BASE_DIR / \"training_ready_dataset.pt\"\n",
    "\n",
    "if True:\n",
    "    dataset = []\n",
    "\n",
    "    def iter_step_files(folder: Path):\n",
    "        return [p for p in folder.rglob('*') if p.suffix.lower() in {'.stp', '.step'}]\n",
    "\n",
    "    possible_heatstakes = iter_step_files(HEATSTAKE_DIR)\n",
    "    possible_others = iter_step_files(OTHER_DIR)\n",
    "\n",
    "    print(f\"Found {len(possible_heatstakes)} heatstake STEP files and {len(possible_others)} other STEP files.\")\n",
    "\n",
    "    for heatstake_path in possible_heatstakes:\n",
    "        solids = cq.importers.importStep(str(heatstake_path)).solids()\n",
    "        for solid in solids:\n",
    "            G = build_brep_graph(solid)\n",
    "            data = nx_to_PyG(G)\n",
    "            data.y = torch.tensor([1], dtype=torch.long)  # class 1 = heatstake\n",
    "            dataset.append(data)\n",
    "\n",
    "    for other_path in possible_others:\n",
    "        solids = cq.importers.importStep(str(other_path)).solids()\n",
    "        for solid in solids:\n",
    "            G = build_brep_graph(solid)\n",
    "            data = nx_to_PyG(G)\n",
    "            data.y = torch.tensor([0], dtype=torch.long)  # class 0 = other\n",
    "            dataset.append(data)\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        print(\"No graphs were created. Ensure your folders contain .stp/.step files and your preprocessing functions are available.\")\n",
    "    else:\n",
    "        torch.save(dataset, DATASET_FILE)\n",
    "        print(f\"Saved dataset with {len(dataset)} graphs to {DATASET_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10664931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (expects a single .pt file saved as a list of PyG Data objects)\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "BASE_DIR = Path(__file__).resolve().parent if '__file__' in globals() else Path.cwd()\n",
    "DATASET_FILE = BASE_DIR / \"training_ready_dataset.pt\"\n",
    "\n",
    "if DATASET_FILE.exists():\n",
    "    dataset = torch.load(DATASET_FILE)\n",
    "    print(f\"Loaded dataset with {len(dataset)} graphs from {DATASET_FILE}\")\n",
    "else:\n",
    "    dataset = []\n",
    "    print(f\"Dataset file not found at {DATASET_FILE}. Add data or build dataset first.\")\n",
    "\n",
    "# Basic sanity check\n",
    "if len(dataset) > 0:\n",
    "    assert hasattr(dataset[0], 'x') and hasattr(dataset[0], 'edge_index') and hasattr(dataset[0], 'y'), \\\n",
    "        \"Each Data must have x, edge_index, and y\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/val and create loaders\n",
    "if len(dataset) > 0:\n",
    "    labels = [int(d.y.item()) for d in dataset]\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        list(range(len(dataset))),\n",
    "        test_size=VAL_SPLIT,\n",
    "        random_state=SEED,\n",
    "        stratify=labels if len(set(labels)) > 1 else None,\n",
    "    )\n",
    "    train_dataset = [dataset[i] for i in train_idx]\n",
    "    val_dataset = [dataset[i] for i in val_idx]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    print(f\"Train graphs: {len(train_dataset)} | Val graphs: {len(val_dataset)}\")\n",
    "else:\n",
    "    train_loader = None\n",
    "    val_loader = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb3f935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GCN import GCN\n",
    "\n",
    "# Create model, criterion, optimizer\n",
    "if train_loader is not None:\n",
    "    in_channels = train_dataset[0].x.size(-1)\n",
    "    model = GCN(feature_dim_size=in_channels, num_classes=2, dropout=DROPOUT).to(DEVICE)\n",
    "    criterion = nn.NLLLoss()  # model returns log_softmax\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    print(model)\n",
    "else:\n",
    "    model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac6309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/eval helpers\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        # The provided GCN does not aggregate per-graph using the batch vector, so\n",
    "        # we process each graph in the batch individually.\n",
    "        data_list = batch.to_data_list()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = 0.0\n",
    "        batch_correct = 0\n",
    "        batch_total = 0\n",
    "\n",
    "        for data in data_list:\n",
    "            data = data.to(DEVICE)\n",
    "            out = model(adj=data.edge_index, features=data.x)  # shape [1, 2]\n",
    "            loss = criterion(out, data.y.long())\n",
    "            loss.backward()\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "            preds = out.argmax(dim=1)\n",
    "            batch_correct += int((preds == data.y).sum().item())\n",
    "            batch_total += data.y.size(0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        correct += batch_correct\n",
    "        total += batch_total\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(loader))\n",
    "    acc = correct / max(1, total)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            data_list = batch.to_data_list()\n",
    "            batch_loss = 0.0\n",
    "            batch_correct = 0\n",
    "            batch_total = 0\n",
    "            for data in data_list:\n",
    "                data = data.to(DEVICE)\n",
    "                out = model(adj=data.edge_index, features=data.x)\n",
    "                loss = criterion(out, data.y.long())\n",
    "                batch_loss += loss.item()\n",
    "                preds = out.argmax(dim=1)\n",
    "                batch_correct += int((preds == data.y).sum().item())\n",
    "                batch_total += data.y.size(0)\n",
    "            total_loss += batch_loss\n",
    "            correct += batch_correct\n",
    "            total += batch_total\n",
    "    avg_loss = total_loss / max(1, len(loader))\n",
    "    acc = correct / max(1, total)\n",
    "    return avg_loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fce4c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with plots\n",
    "if train_loader is not None and model is not None:\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        vl_loss, vl_acc = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_loss\"].append(vl_loss)\n",
    "        history[\"val_acc\"].append(vl_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | Train Loss: {tr_loss:.4f} Acc: {tr_acc:.3f} | Val Loss: {vl_loss:.4f} Acc: {vl_acc:.3f}\")\n",
    "\n",
    "    # Plot loss and accuracy\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    axs[0].plot(history[\"train_loss\"], label=\"train\")\n",
    "    axs[0].plot(history[\"val_loss\"], label=\"val\")\n",
    "    axs[0].set_title(\"Loss\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"NLLLoss\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(history[\"train_acc\"], label=\"train\")\n",
    "    axs[1].plot(history[\"val_acc\"], label=\"val\")\n",
    "    axs[1].set_title(\"Accuracy\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No dataset loaded. Build or load heatstake_dataset.pt first.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heatstakes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
