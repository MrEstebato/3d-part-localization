{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983687be",
   "metadata": {},
   "source": [
    "# Train GCN for Heat Stake Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5421503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c6c074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "SEED = 60\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 16\n",
    "LR = 0.001\n",
    "DROPOUT = 0.3\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(torch.cuda.get_arch_list())\n",
    "print(torch.cuda.is_available())\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce416e45",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b24ca4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cadquery as cq\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "\n",
    "# Resolve paths relative to this notebook's folder when possible\n",
    "BASE_DIR = Path.cwd().parent\n",
    "if str(BASE_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(BASE_DIR))\n",
    "    \n",
    "from preprocessing.graphs import build_brep_graph, nx_to_PyG\n",
    "\n",
    "DATA_DIR = BASE_DIR / \"GCN\" / \"training_data\"\n",
    "HEATSTAKE_DIR = DATA_DIR / \"heatstakes\"\n",
    "OTHER_DIR = DATA_DIR / \"other\"\n",
    "DATASET_FILE = BASE_DIR / \"GCN\" / \"training_ready_dataset.pt\"\n",
    "\n",
    "if False:\n",
    "    dataset = []\n",
    "\n",
    "    def iter_step_files(folder: Path):\n",
    "        return [p for p in folder.rglob('*') if p.suffix.lower() in {'.stp', '.step'}]\n",
    "\n",
    "    possible_heatstakes = iter_step_files(HEATSTAKE_DIR)\n",
    "    possible_others = iter_step_files(OTHER_DIR)\n",
    "\n",
    "    print(f\"Found {len(possible_heatstakes)} heatstake STEP files and {len(possible_others)} other STEP files.\")\n",
    "    for heatstake_path in possible_heatstakes:\n",
    "        solids = cq.importers.importStep(str(heatstake_path)).faces()\n",
    "        # for solid in solids.all():\n",
    "        #     G = build_brep_graph(solid)\n",
    "        #     data = nx_to_PyG([G])\n",
    "        #     #print(type(data[0]))\n",
    "        #     data[0].y = torch.tensor([1], dtype=torch.long)  # class 1 = heatstake\n",
    "        #     dataset.append(data[0])\n",
    "        G = build_brep_graph(solids)\n",
    "        data = nx_to_PyG([G])\n",
    "        data[0].y = torch.tensor([1], dtype=torch.long)  # class 1 = heatstake\n",
    "        dataset.append(data[0])\n",
    "    for other_path in possible_others:\n",
    "        solids = cq.importers.importStep(str(other_path)).faces()\n",
    "        # for solid in solids.all():\n",
    "        #     G = build_brep_graph(solid)\n",
    "        #     data = nx_to_PyG([G])\n",
    "        #     data[0].y = torch.tensor([0], dtype=torch.long)  # class 0 = other\n",
    "        #     dataset.append(data[0])        \n",
    "        G = build_brep_graph(solids)\n",
    "        data = nx_to_PyG([G])\n",
    "        data[0].y = torch.tensor([0], dtype=torch.long)  # class 0 = other\n",
    "        dataset.append(data[0])\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        print(\"No graphs were created. Ensure your folders contain .stp/.step files and your preprocessing functions are available.\")\n",
    "    else:\n",
    "        torch.save(dataset, DATASET_FILE)\n",
    "        print(f\"Saved dataset with {len(dataset)} graphs to {DATASET_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10664931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 64 graphs from c:\\Users\\A01369877\\Documents\\GM\\3d-part-localization\\GCN\\training_ready_dataset.pt\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (expects a single .pt file saved as a list of PyG Data objects)\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATASET_FILE = BASE_DIR / \"GCN\" / \"training_ready_dataset.pt\"\n",
    "\n",
    "if DATASET_FILE.exists():\n",
    "    dataset = torch.load(DATASET_FILE, weights_only=False)\n",
    "    print(f\"Loaded dataset with {len(dataset)} graphs from {DATASET_FILE}\")\n",
    "else:\n",
    "    dataset = []\n",
    "    print(f\"Dataset file not found at {DATASET_FILE}. Add data or build dataset first.\")\n",
    "\n",
    "# Basic sanity check\n",
    "if len(dataset) > 0:\n",
    "    assert hasattr(dataset[0], 'x') and hasattr(dataset[0], 'edge_index') and hasattr(dataset[0], 'y'), \\\n",
    "        \"Each Data must have x, edge_index, and y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db54de4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train graphs: 51 | Val graphs: 13\n"
     ]
    }
   ],
   "source": [
    "# Split into train/val and create loaders\n",
    "if len(dataset) > 0:\n",
    "    labels = [int(d.y.item()) for d in dataset]\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        list(range(len(dataset))),\n",
    "        test_size=VAL_SPLIT,\n",
    "        random_state=SEED,\n",
    "        stratify=labels if len(set(labels)) > 1 else None,\n",
    "    )\n",
    "    train_dataset = [dataset[i] for i in train_idx]\n",
    "    val_dataset = [dataset[i] for i in val_idx]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    print(f\"Train graphs: {len(train_dataset)} | Val graphs: {len(val_dataset)}\")\n",
    "else:\n",
    "    train_loader = None\n",
    "    val_loader = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2cb3f935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (convolution_1): GCNConv(3, 64)\n",
      "  (convolution_2): GCNConv(64, 32)\n",
      "  (convolution_3): GCNConv(32, 32)\n",
      "  (attention): AttentionModule()\n",
      "  (fully_connected_first): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fully_connected_second): Linear(in_features=32, out_features=8, bias=True)\n",
      "  (scoring_layer): Linear(in_features=8, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from GCN import GCN\n",
    "\n",
    "# Create model, criterion, optimizer\n",
    "if train_loader is not None:\n",
    "    in_channels = train_dataset[0].x.size(-1)\n",
    "    model = GCN(feature_dim_size=in_channels, num_classes=2, dropout=DROPOUT).to(DEVICE)\n",
    "    criterion = nn.NLLLoss()  # model returns log_softmax\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    print(model)\n",
    "else:\n",
    "    model = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ac6309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/eval helpers\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        # The provided GCN does not aggregate per-graph using the batch vector, so\n",
    "        # we process each graph in the batch individually.\n",
    "        data_list = batch.to_data_list()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = 0.0\n",
    "        batch_correct = 0\n",
    "        batch_total = 0\n",
    "\n",
    "        for data in data_list:\n",
    "            data = data.to(DEVICE)\n",
    "            out = model(adj=data.edge_index, features=data.x)  # shape [1, 2]\n",
    "            loss = criterion(out, data.y.long())\n",
    "            loss.backward()\n",
    "            batch_loss += loss.item()\n",
    "\n",
    "            preds = out.argmax(dim=1)\n",
    "            batch_correct += int((preds == data.y).sum().item())\n",
    "            batch_total += data.y.size(0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        correct += batch_correct\n",
    "        total += batch_total\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(loader))\n",
    "    acc = correct / max(1, total)\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            data_list = batch.to_data_list()\n",
    "            batch_loss = 0.0\n",
    "            batch_correct = 0\n",
    "            batch_total = 0\n",
    "            for data in data_list:\n",
    "                data = data.to(DEVICE)\n",
    "                out = model(adj=data.edge_index, features=data.x)\n",
    "                loss = criterion(out, data.y.long())\n",
    "                batch_loss += loss.item()\n",
    "                preds = out.argmax(dim=1)\n",
    "                batch_correct += int((preds == data.y).sum().item())\n",
    "                batch_total += data.y.size(0)\n",
    "            total_loss += batch_loss\n",
    "            correct += batch_correct\n",
    "            total += batch_total\n",
    "    avg_loss = total_loss / max(1, len(loader))\n",
    "    acc = correct / max(1, total)\n",
    "    return avg_loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9fce4c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Loss: 13.4599 Acc: 0.451 | Val Loss: 8.8508 Acc: 0.615\n",
      "Epoch 002 | Train Loss: 8.9904 Acc: 0.588 | Val Loss: 9.1407 Acc: 0.615\n",
      "Epoch 003 | Train Loss: 9.0481 Acc: 0.588 | Val Loss: 9.0734 Acc: 0.615\n",
      "Epoch 004 | Train Loss: 8.8955 Acc: 0.588 | Val Loss: 8.9886 Acc: 0.615\n",
      "Epoch 005 | Train Loss: 8.8127 Acc: 0.588 | Val Loss: 8.8881 Acc: 0.615\n",
      "Epoch 006 | Train Loss: 8.6228 Acc: 0.588 | Val Loss: 8.8351 Acc: 0.615\n",
      "Epoch 007 | Train Loss: 8.5897 Acc: 0.588 | Val Loss: 8.8119 Acc: 0.615\n",
      "Epoch 008 | Train Loss: 8.6490 Acc: 0.588 | Val Loss: 8.8156 Acc: 0.615\n",
      "Epoch 009 | Train Loss: 8.6172 Acc: 0.588 | Val Loss: 8.8209 Acc: 0.615\n",
      "Epoch 010 | Train Loss: 8.5920 Acc: 0.588 | Val Loss: 8.8264 Acc: 0.615\n",
      "Epoch 011 | Train Loss: 8.5860 Acc: 0.588 | Val Loss: 8.8308 Acc: 0.615\n",
      "Epoch 012 | Train Loss: 8.5888 Acc: 0.588 | Val Loss: 8.8305 Acc: 0.615\n",
      "Epoch 013 | Train Loss: 8.5938 Acc: 0.588 | Val Loss: 8.8287 Acc: 0.615\n",
      "Epoch 014 | Train Loss: 8.5891 Acc: 0.588 | Val Loss: 8.8231 Acc: 0.615\n",
      "Epoch 015 | Train Loss: 8.5790 Acc: 0.588 | Val Loss: 8.8185 Acc: 0.615\n",
      "Epoch 016 | Train Loss: 8.5779 Acc: 0.588 | Val Loss: 8.8150 Acc: 0.615\n",
      "Epoch 017 | Train Loss: 8.5808 Acc: 0.588 | Val Loss: 8.8118 Acc: 0.615\n",
      "Epoch 018 | Train Loss: 8.5767 Acc: 0.588 | Val Loss: 8.8100 Acc: 0.615\n",
      "Epoch 019 | Train Loss: 8.5863 Acc: 0.588 | Val Loss: 8.8089 Acc: 0.615\n",
      "Epoch 020 | Train Loss: 8.5845 Acc: 0.588 | Val Loss: 8.8071 Acc: 0.615\n",
      "Epoch 021 | Train Loss: 8.5788 Acc: 0.588 | Val Loss: 8.8077 Acc: 0.615\n",
      "Epoch 022 | Train Loss: 8.5857 Acc: 0.588 | Val Loss: 8.8057 Acc: 0.615\n",
      "Epoch 023 | Train Loss: 8.5775 Acc: 0.588 | Val Loss: 8.8051 Acc: 0.615\n",
      "Epoch 024 | Train Loss: 8.5759 Acc: 0.588 | Val Loss: 8.8039 Acc: 0.615\n",
      "Epoch 025 | Train Loss: 8.5718 Acc: 0.588 | Val Loss: 8.8012 Acc: 0.615\n",
      "Epoch 026 | Train Loss: 8.5788 Acc: 0.588 | Val Loss: 8.7980 Acc: 0.615\n",
      "Epoch 027 | Train Loss: 8.5802 Acc: 0.588 | Val Loss: 8.7963 Acc: 0.615\n",
      "Epoch 028 | Train Loss: 8.5749 Acc: 0.588 | Val Loss: 8.7941 Acc: 0.615\n",
      "Epoch 029 | Train Loss: 8.5756 Acc: 0.588 | Val Loss: 8.7923 Acc: 0.615\n",
      "Epoch 030 | Train Loss: 8.5713 Acc: 0.588 | Val Loss: 8.7891 Acc: 0.615\n",
      "Epoch 031 | Train Loss: 8.5774 Acc: 0.588 | Val Loss: 8.7857 Acc: 0.615\n",
      "Epoch 032 | Train Loss: 8.5655 Acc: 0.588 | Val Loss: 8.7835 Acc: 0.615\n",
      "Epoch 033 | Train Loss: 8.5715 Acc: 0.588 | Val Loss: 8.7809 Acc: 0.615\n",
      "Epoch 034 | Train Loss: 8.5703 Acc: 0.588 | Val Loss: 8.7787 Acc: 0.615\n",
      "Epoch 035 | Train Loss: 8.5737 Acc: 0.588 | Val Loss: 8.7775 Acc: 0.615\n",
      "Epoch 036 | Train Loss: 8.5729 Acc: 0.588 | Val Loss: 8.7782 Acc: 0.615\n",
      "Epoch 037 | Train Loss: 8.5817 Acc: 0.588 | Val Loss: 8.7798 Acc: 0.615\n",
      "Epoch 038 | Train Loss: 8.5720 Acc: 0.588 | Val Loss: 8.7769 Acc: 0.615\n",
      "Epoch 039 | Train Loss: 8.5712 Acc: 0.588 | Val Loss: 8.7753 Acc: 0.615\n",
      "Epoch 040 | Train Loss: 8.5733 Acc: 0.588 | Val Loss: 8.7722 Acc: 0.615\n",
      "Epoch 041 | Train Loss: 8.5698 Acc: 0.588 | Val Loss: 8.7651 Acc: 0.615\n",
      "Epoch 042 | Train Loss: 8.5732 Acc: 0.588 | Val Loss: 8.7609 Acc: 0.615\n",
      "Epoch 043 | Train Loss: 8.5701 Acc: 0.588 | Val Loss: 8.7593 Acc: 0.615\n",
      "Epoch 044 | Train Loss: 8.5692 Acc: 0.588 | Val Loss: 8.7566 Acc: 0.615\n",
      "Epoch 045 | Train Loss: 8.5674 Acc: 0.588 | Val Loss: 8.7537 Acc: 0.615\n",
      "Epoch 046 | Train Loss: 8.5678 Acc: 0.588 | Val Loss: 8.7509 Acc: 0.615\n",
      "Epoch 047 | Train Loss: 8.5510 Acc: 0.588 | Val Loss: 8.7507 Acc: 0.615\n",
      "Epoch 048 | Train Loss: 8.5611 Acc: 0.588 | Val Loss: 8.7498 Acc: 0.615\n",
      "Epoch 049 | Train Loss: 8.5653 Acc: 0.588 | Val Loss: 8.7469 Acc: 0.615\n",
      "Epoch 050 | Train Loss: 8.5624 Acc: 0.588 | Val Loss: 8.7433 Acc: 0.615\n",
      "Epoch 051 | Train Loss: 8.5625 Acc: 0.588 | Val Loss: 8.7412 Acc: 0.615\n",
      "Epoch 052 | Train Loss: 8.5619 Acc: 0.588 | Val Loss: 8.7378 Acc: 0.615\n",
      "Epoch 053 | Train Loss: 8.5565 Acc: 0.588 | Val Loss: 8.7370 Acc: 0.615\n",
      "Epoch 054 | Train Loss: 8.5555 Acc: 0.588 | Val Loss: 8.7335 Acc: 0.615\n",
      "Epoch 055 | Train Loss: 8.5547 Acc: 0.588 | Val Loss: 8.7295 Acc: 0.615\n",
      "Epoch 056 | Train Loss: 8.5685 Acc: 0.588 | Val Loss: 8.7277 Acc: 0.615\n",
      "Epoch 057 | Train Loss: 8.5615 Acc: 0.588 | Val Loss: 8.7251 Acc: 0.615\n",
      "Epoch 058 | Train Loss: 8.5573 Acc: 0.588 | Val Loss: 8.7233 Acc: 0.615\n",
      "Epoch 059 | Train Loss: 8.5551 Acc: 0.588 | Val Loss: 8.7185 Acc: 0.615\n",
      "Epoch 060 | Train Loss: 8.5553 Acc: 0.588 | Val Loss: 8.7133 Acc: 0.615\n",
      "Epoch 061 | Train Loss: 8.5528 Acc: 0.588 | Val Loss: 8.7123 Acc: 0.615\n",
      "Epoch 062 | Train Loss: 8.5519 Acc: 0.588 | Val Loss: 8.7095 Acc: 0.615\n",
      "Epoch 063 | Train Loss: 8.5516 Acc: 0.588 | Val Loss: 8.7053 Acc: 0.615\n",
      "Epoch 064 | Train Loss: 8.5511 Acc: 0.588 | Val Loss: 8.6989 Acc: 0.615\n",
      "Epoch 065 | Train Loss: 8.5566 Acc: 0.588 | Val Loss: 8.6928 Acc: 0.615\n",
      "Epoch 066 | Train Loss: 8.5420 Acc: 0.588 | Val Loss: 8.7019 Acc: 0.615\n",
      "Epoch 067 | Train Loss: 8.5482 Acc: 0.588 | Val Loss: 8.6902 Acc: 0.615\n",
      "Epoch 068 | Train Loss: 8.5467 Acc: 0.588 | Val Loss: 8.6924 Acc: 0.615\n",
      "Epoch 069 | Train Loss: 8.5448 Acc: 0.588 | Val Loss: 8.6936 Acc: 0.615\n",
      "Epoch 070 | Train Loss: 8.5547 Acc: 0.588 | Val Loss: 8.6918 Acc: 0.615\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m history \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: [], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: []}\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 6\u001b[0m     tr_loss, tr_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     vl_loss, vl_acc \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion)\n\u001b[0;32m      9\u001b[0m     history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(tr_loss)\n",
      "Cell \u001b[1;32mIn[60], line 21\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_list:\n\u001b[0;32m     20\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 21\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape [1, 2]\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mlong())\n\u001b[0;32m     23\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1772\u001b[0m             args, kwargs \u001b[38;5;241m=\u001b[39m args_kwargs_result\n\u001b[0;32m   1773\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1774\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m-> 1775\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward pre-hook must return None or a tuple \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1776\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs_kwargs_result\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1777\u001b[0m             )\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1779\u001b[0m     args_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args)\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1783\u001b[0m                 args \u001b[38;5;241m=\u001b[39m args_result\n\u001b[0;32m   1785\u001b[0m bw_hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m backward_pre_hooks:\n\u001b[0;32m   1787\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\Documents\\GM\\3d-part-localization\\GCN\\GCN.py:63\u001b[0m, in \u001b[0;36mGCN.forward\u001b[1;34m(self, adj, features)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, adj, features):\n\u001b[1;32m---> 63\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolution_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m     features \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(features)\n\u001b[0;32m     65\u001b[0m     features \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(\n\u001b[0;32m     66\u001b[0m         features, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[0;32m     67\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1772\u001b[0m             args, kwargs \u001b[38;5;241m=\u001b[39m args_kwargs_result\n\u001b[0;32m   1773\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1774\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m-> 1775\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward pre-hook must return None or a tuple \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1776\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs_kwargs_result\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1777\u001b[0m             )\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1779\u001b[0m     args_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args)\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1783\u001b[0m                 args \u001b[38;5;241m=\u001b[39m args_result\n\u001b[0;32m   1785\u001b[0m bw_hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m backward_pre_hooks:\n\u001b[0;32m   1787\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:241\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    239\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 241\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43mgcn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# yapf: disable\u001b[39;49;00m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimproved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_self_loops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached:\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_edge_index \u001b[38;5;241m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:99\u001b[0m, in \u001b[0;36mgcn_norm\u001b[1;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m maybe_num_nodes(edge_index, num_nodes)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_self_loops:\n\u001b[1;32m---> 99\u001b[0m     edge_index, edge_weight \u001b[38;5;241m=\u001b[39m \u001b[43madd_remaining_self_loops\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     edge_weight \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), ), dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    104\u001b[0m                              device\u001b[38;5;241m=\u001b[39medge_index\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch_geometric\\utils\\loop.py:627\u001b[0m, in \u001b[0;36madd_remaining_self_loops\u001b[1;34m(edge_index, edge_attr, fill_value, num_nodes)\u001b[0m\n\u001b[0;32m    624\u001b[0m mask \u001b[38;5;241m=\u001b[39m edge_index[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m edge_index[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    626\u001b[0m device \u001b[38;5;241m=\u001b[39m edge_index\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m--> 627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_scripting\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, EdgeIndex):\n\u001b[0;32m    628\u001b[0m     loop_index: Tensor \u001b[38;5;241m=\u001b[39m EdgeIndex(\n\u001b[0;32m    629\u001b[0m         torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, N, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    630\u001b[0m         sparse_size\u001b[38;5;241m=\u001b[39m(N, N),\n\u001b[0;32m    631\u001b[0m         is_undirected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    632\u001b[0m     )\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\A01369877\\AppData\\Local\\miniconda3\\envs\\heatstakes\\lib\\site-packages\\torch\\_jit_internal.py:106\u001b[0m, in \u001b[0;36mis_scripting\u001b[1;34m()\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_scripting\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    Function that returns True when in compilation and False otherwise. This\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;124;03m    is useful especially with the @unused decorator to leave code in your\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m    model that is not yet TorchScript compatible.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    .. testcode::\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \n\u001b[0;32m    110\u001b[0m \u001b[38;5;124;03m        import torch\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m        @torch.jit.unused\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m        def unsupported_linear_op(x):\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m            return x\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m        def linear(x):\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m            if torch.jit.is_scripting():\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m                return torch.linear(x)\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m            else:\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m                return unsupported_linear_op(x)\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop with plots\n",
    "if train_loader is not None and model is not None:\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        vl_loss, vl_acc = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_loss\"].append(vl_loss)\n",
    "        history[\"val_acc\"].append(vl_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | Train Loss: {tr_loss:.4f} Acc: {tr_acc:.3f} | Val Loss: {vl_loss:.4f} Acc: {vl_acc:.3f}\")\n",
    "\n",
    "    # Plot loss and accuracy\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    axs[0].plot(history[\"train_loss\"], label=\"train\")\n",
    "    axs[0].plot(history[\"val_loss\"], label=\"val\")\n",
    "    axs[0].set_title(\"Loss\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"NLLLoss\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(history[\"train_acc\"], label=\"train\")\n",
    "    axs[1].plot(history[\"val_acc\"], label=\"val\")\n",
    "    axs[1].set_title(\"Accuracy\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No dataset loaded. Build or load heatstake_dataset.pt first.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heatstakes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
